
\documentclass[12pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{soul}
\usepackage{float}
\usepackage{listings}

\title{Information Theory Homework 2}
\author{Sebastian HÃ¶rl\\Personal number: 9007126130\\E-Mail: hoerl.sebastian@gmail.com}
\date{\today}

\lstset{ %
  basicstyle=\footnotesize\ttfamily    
}

\begin{document}
\maketitle

The task is to find the correlation complexity $\eta$ for a given Hidden Markov Model:

\begin{figure}[!h]
\centering
\includegraphics[width=0.2\textwidth]{hmm}
\end{figure}

At first one can construct the underlying Markov model with the 5 states A, B, C, D and E. The HMM output, that is generated by reaching the note is denoted next to the node name and the transition probabilities are denoted at the connections (if two connections leave one node, equal probability is assumed as stated in the task description):

\begin{figure}[!h]
\centering
\includegraphics[width=0.3\textwidth]{mm}
\end{figure}

\pagebreak
\section{Conditional Probability Approach}

The idea in this approach is to first have no knowledge about the distribution of zeros and ones and therfore assume equal probability. As the next step the underlying Markov model is examined and a stationary distribution of the states is derived, which can be translated back to a stationary distribution of the HMM outputs (0/1). Then subsequentially one more sign is read and using the Markov model the conditional probability is derived of observing a 0 or 1 after the already known sequence. Then comparing the probability of the current sequence with the one where the first read symbol is left out, one can compute the correlation information (KL divergence) for a certain correlation length. These correlation informations are then used in order to define the correlation complexity $\eta$.


\subsection{Steady state probabilities \& Density information}

Taking the probabilities, from the Markov model, one can setup the following relations:

\begin{equation}\begin{aligned}
P(A) &= P(D)\ \ \ P(B)=\frac{1}{2}P(A)\ \ \ P(C)=P(B) + P(C)\\
P(D) &= P(C)\ \ \ P(E) = \frac{1}{2}P(A)
\end{aligned}\end{equation}

Also one knows that the sum must be normed to one:

\begin{equation}\begin{aligned}
\sum_{Z\in\{A,B,C,D,E\}} P(Z) &= 1
&= 2 P(A) + \frac{4}{2} P(A)
\end{aligned}\end{equation}

So one arrives at:

\begin{table}[!h]
\centering
\begin{tabular}{l||c|c|c|c|c}
$Z$ & $A$ & $B$ & $C$ & $D$ & $E$\\ \hline
$P(Z)$ & $\frac{1}{4}$ & $\frac{1}{8}$ & $\frac{1}{4}$ & $\frac{1}{4}$ & $\frac{1}{8}$
\end{tabular}
\end{table}

This is the distribution of states observed. In order to determine the distribution of the 0/1 outputs, one computes the sum of the respective generating nodes:

\begin{equation}\begin{aligned}
P^1(0) &= P(C) + P(D) + P(E) = \frac{5}{8}\\
P^1(1) &= P(A) + P(B) = \frac{3}{8}
\end{aligned}\end{equation}

This is the posterior density for observing a position in the lattice. The prior is to assume maximum entropy and therefore:

\begin{equation}
P^0(0) = P^0(1) = \frac{1}{2}
\end{equation}

The density information is then:

\begin{equation}\begin{aligned}
k_1 &= P^1(0) \log \frac{P^1(0)}{P^0(0)} + P^1(1) \log \frac{P^1(1)}{P^0(1)}\\
&= \frac{5}{8} \log \frac{5}{4} + \frac{3}{8} \log \frac{3}{4}\\
& = 0.0456
\end{aligned}\end{equation}

\subsection{2-step correlation}

Now $P(0|0)$ denotes ``observing 0, after in the first step a zero has been observed'', so one needs to have a look at the Markov model, find all occurences of a path, where a zero follows a zero:

\begin{equation}
P(0|0) = \frac{P(C|E)P(E)+P(D|C)P(C)}{P(0)} = \frac{\frac{1}{8} + \frac{1}{4}}{\frac{5}{8}} = \frac{3}{5}
\end{equation}

Basically what one has to do is to look at every starting node in the Markov model and try to find a path that complies to ``0 after 0''. Then one follows this path, and based on the stationary probability of the starting node and the transition probabilities one finds the probability to follow this path. All possible paths that match the current scenario are summed up and in order to get the probability of making this transition after already having seen a path (here the previous zero) one needs to divide by the probability of observing the preceding path itself. This can be done for the other probabilities as well, while symmetries can be used:

\begin{equation}\begin{aligned}
P(0|0) & &= \frac{3}{5}\\
P(1|0) &= 1 - P(0|0) &= \frac{2}{5}\\
P(0|1) &= \frac{P(C|B)P(B) + P(C|A)P(A)}{P(1)} = \frac{\frac{1}{8} + \frac{1}{2}\frac{1}{4}}{\frac{3}{8}} &= \frac{2}{3}\\
P(1|1) &= 1 - P(0|1) &= \frac{1}{3}
\end{aligned}\end{equation} 

In order to compute the correlation information one first computes the KL divergence between the distribution of 0 and 1 in the current observed step with the full preceding sequence in comparison to the preceding sequence after leaving out the first observed symbol. This is done for each possible preceding sequence and then all the values are averaged using the probability of those sequences:

\begin{equation}\begin{aligned}
k_2 &= P(0)\left[ 
P(0|0) \log \frac{P(0|0)}{P(0)} + P(1|0) \log \frac{P(1|0)}{P(1)}
\right]\\
&+ P(1)\left[ 
P(0|1) \log \frac{P(0|1)}{P(0)} + P(1|1) \log \frac{P(1|1)}{P(1)}
\right]\\
&= \frac{5}{8} \left( 
	\frac{3}{5} \log \frac{\frac{3}{5}}{\frac{5}{8}} + \frac{2}{5}\log\frac{\frac{2}{5}}{\frac{3}{8}}
\right) + 
\frac{3}{8} \left(
	\frac{2}{3} \log \frac{\frac{2}{3}}{\frac{5}{8}} + \frac{1}{3} \log \frac{\frac{1}{3}}{\frac{3}{8}}
\right)\\
&= 0.0033
\end{aligned}\end{equation}

\subsection{3-step correlations}

In order to get the 3-step correlations one first needs to compute the total probabilities of observing a sequence:

\begin{equation}\begin{aligned}
P(00) &= P(0|0) P(0) &= \frac{3}{5} \cdot \frac{5}{8} &= \frac{3}{8}\\
P(01) &= P(1|0) P(0) &= \frac{2}{5} \cdot \frac{5}{8} &= \frac{2}{8}\\
P(10) &= P(0|1) P(1) &= \frac{2}{3} \cdot \frac{3}{8} &= \frac{2}{8}\\
P(11) &= P(1|1) P(1) &= \frac{1}{3} \cdot \frac{3}{8} &= \frac{1}{8}\\
\end{aligned}\end{equation}

Now, without writing out all the transitions in the Markov model, the conditional probabilities are:

\begin{equation}\begin{aligned}
P(0|00) &= \frac{P(E)}{P(00)} &= \frac{1}{3}\\
P(1|00) &= &= \frac{2}{3}\\
P(0|01) &= \frac{\frac{1}{2}P(D)}{P(01)} &= \frac{1}{2}\\
P(1|01) &= &= \frac{1}{2}\\
P(0|10) &= \frac{P(B) + \frac{1}{2}P(A)}{P(10)} &= 1\\
P(1|10) &= &= 0\\
P(0|11) &= \frac{\frac{1}{2}P(A)}{P(11)} &= 1\\
P(1|11) &= &= 0
\end{aligned}\end{equation}

The correlation information is calculated in the same procedure as before. Writing it out (for the last time) gives:

\begin{equation}\begin{aligned}
k_3 
&= P(00) \left( P(0|00) \log \frac{P(0|00)}{P(0|0)} + P(1|00) \log \frac{P(1|00)}{P(1|0)} \right)\\
&+ P(01) \left( P(0|01) \log \frac{P(0|01)}{P(0|1)} + P(1|00) \log \frac{P(1|01)}{P(1|1)} \right)\\
&+ P(10) \left( P(0|10) \log \frac{P(0|10)}{P(0|0)} + P(1|10) \log \frac{P(1|10)}{P(1|0)} \right)\\
&+ P(11) \left( P(0|11) \log \frac{P(0|11)}{P(0|1)} + P(1|11) \log \frac{P(1|11)}{P(1|1)} \right)\\
&= \frac{3}{8} \left( 
	\frac{1}{3} \log \frac{\frac{1}{3}}{\frac{3}{5}} +
	\frac{2}{3} \log \frac{\frac{2}{3}}{\frac{2}{5}}
\right) + \frac{2}{8} \left( 
	\frac{1}{2} \log \frac{\frac{1}{2}}{\frac{2}{3}} +
	\frac{1}{2} \log \frac{\frac{1}{2}}{\frac{1}{3}}
\right)\\
&+ \frac{2}{8} \log \frac{5}{3} + \frac{1}{8} \log \frac{3}{2}\\
&= 0.3568
\end{aligned}\end{equation}

\subsection{4-step correlations}

Again, first the total probabilities for the 3-step sequences need to be calculated. The ones that have a zero probability already are omitted:

\begin{table}[!h]
\centering
\begin{tabular}{c||c|c|c|c|c|c}
$\sigma$ & 000 & 001 & 010 & 011 & 100 & 110 \\ \hline
$P(\sigma)$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$
\end{tabular}
\end{table}

Again, the conditional probabilities will be determined. Here all cases that lead to zero probability are omitted, because they will not have any impact one the correlation information:

\begin{equation}\begin{aligned}
P(1|000) &= \frac{P(E)}{P(000)} &= 1\\
P(0|001) &= \frac{\frac{1}{2}P(C)}{P(001)} &= \frac{1}{2}\\
P(1|001) &= &= \frac{1}{2}\\
P(0|010) &= \frac{\frac{1}{2}P(D)}{P(010)} &= 1\\
P(0|011) &= \frac{\frac{1}{2}P(D)}{P(011)} &= 1\\
P(0|100) &= \frac{\frac{1}{2}P(A)}{P(100)} &= \frac{1}{2}\\
P(1|100) &= &= \frac{1}{2}\\
P(0|110) &= \frac{\frac{1}{2}P(A)}{P(110)} &= 1
\end{aligned}\end{equation}

The correlation information is:

\begin{equation}\begin{aligned}
k_4 &= \frac{1}{8} \log \frac{3}{2}
+ \frac{1}{8} \left( \log \frac{\frac{1}{2}}{\frac{1}{3}} + \log \frac{\frac{1}{2}}{\frac{2}{3}} \right)\\
&= 0.0944
\end{aligned}\end{equation}

\subsection{5-step correlations}

It turns out, that in the 5th step all total probabilities for the remaining 4-step sequences are the same:

\begin{equation}\begin{aligned}
Z &= \left\{
0001, 0010, 0011, 0100, 0110, 1000, 1001, 1100
\right\}\\
P(\sigma) &= \frac{1}{8} \ \ \forall \sigma \in Z
\end{aligned}\end{equation}

Calculating the conditional probabilities just like before results in:

\begin{table}[!h]
\centering
\begin{tabular}{c||c|c|c|c|c|c|c|c}
$\sigma$ & 0001 & 0010 & 0011 & 0100 & 0110 & 1000 & 1001 & 1100\\ \hline
$P(0|\sigma)$ & $\frac{1}{2}$ & $1$ & $1$ & $1$ & $1$ & $0$ & $\frac{1}{2}$ & $1$\\ \hline
$P(1|\sigma)$ & $\frac{1}{2}$ & $0$ & $0$ & $0$ & $0$ & $1$ & $\frac{1}{2}$ & $0$
\end{tabular}
\end{table}

The correlation information becomes:

\begin{equation}\begin{aligned}
k_5 &= \frac{2}{8} \log 2 = 0.25
\end{aligned}\end{equation}

\subsection{6-step correlations}

The remaining sequences that can be observed are:

\begin{equation}\begin{aligned}
Z &= \left\{
00010, 00011, 10011, 10010
\right\}\\
P(\sigma) &= \frac{1}{16} \ \ \forall \sigma \in Z
\end{aligned}\end{equation}

The conditional probabilities are:

\begin{table}[!h]
\centering
\begin{tabular}{c||c|c|c|c}
$\sigma$ & 00010 & 00011 & 10011 & 10010\\ \hline
$P(0|\sigma)$ & $1$ & $1$ & $1$ & $1$\\ \hline
$P(1|\sigma)$ & $0$ & $0$ & $0$ & $0$
\end{tabular}
\end{table}

This means that for each remaining sequence, revealing one more symbol does not give any news information. One already knows which symbol will follow for all of these four remaining sequences. Consequentially the correlation information for 6-step sequences is zero:

\begin{equation}
k_6 = 0
\end{equation}

So the model has a correlation length of 5 steps.

\subsection{Correlation Complexity}

Now the correlation complexity

\begin{equation}
\eta = \sum_{m=1}^\infty (m-1)k_m
\end{equation}

can easily be computed, because all correlation values for $m>5$ will be zero. As a summary the computed correlation information was:

\begin{table}[!h]
\centering
\begin{tabular}{c||c|c|c|c|c|c}
$m$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$\\ \hline
$k_m$ & $0.0456$ & $0.0033$ & $0.3568$ & $0.0944$ & $0.25$ & $0$
\end{tabular}
\end{table}

The resulting value is:

\begin{equation}
\eta = 2
\end{equation}

\section{Block Entropy Approach}

Another approach is to calculate the block entropies of the system. This means that one looks at the output of a system and then finds the probability distribution of the possible sequences of length $m$. For each of these distributions the entropy $S_m$ (which is called block entropy of length $m$) can be computed. Form these block entropies the correlation information can be derived by:

\begin{equation}
k_m = -S_{m} + 2 S_{m-1} - S_{m-2}
\end{equation}

\subsection{Simulation}

The given model has been simulated\footnote{
The source code can be found at \texttt{https://github.com/blogsh/it2}} to generate a sequence of $1000$ symbols. Then the block entropies have been measured and the correlation values have been computed. The results can be seen in figure \ref{fig:plot}. The actual values are:

\begin{table}[!h]
\centering
\begin{tabular}{c||c|c|c|c|c|c}
$m$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$\\ \hline
$k_m$ & $0.0437$ & $0.0030$ & $0.3631$ & $0.0900$ & $0.2500$ & $0$
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{plot}
\caption{Top: Block entropies, Bottom: correlation information}
\label{fig:plot}
\end{figure}

The resulting correlation complexity is also $\eta = 2$. The values differ slightly from the analytical results, since the whole analysis is performed on a finite lattice with $1000$ elements. However, it can be seen that they give quite a good approximation to the theoretical results.

\end{document}
